{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c3e8877",
   "metadata": {},
   "source": [
    "# 3-1번\n",
    "어느 학교에서 추출한 55명 학생들의 키 정보이다.\n",
    "이 학생들의 키의 95% 신뢰구간을 구하고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a391d7ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>user_1</td>\n",
       "      <td>164.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>user_2</td>\n",
       "      <td>174.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>user_3</td>\n",
       "      <td>171.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>user_4</td>\n",
       "      <td>162.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>user_5</td>\n",
       "      <td>167.11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ID  height\n",
       "0  user_1  164.57\n",
       "1  user_2  174.99\n",
       "2  user_3  171.41\n",
       "3  user_4  162.47\n",
       "4  user_5  167.11"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e5_p3_1.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38f53cc",
   "metadata": {},
   "source": [
    "# 3-1-a\n",
    "55명 학생들의 키에 대한 표본 평균을 구하여라(반올림하여 소숫점 3째자리까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d7c4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169.937\n"
     ]
    }
   ],
   "source": [
    "print(round(df['height'].mean(), 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25338769",
   "metadata": {},
   "source": [
    "# 3-1-b\n",
    "t분포 양쪽 꼬리에서의 t 값을 구하여라 (반올림하여 소수4째자리까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dad6c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfd5444f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ConstantInputWarning', 'DegenerateDataWarning', 'FitError', 'NearConstantInputWarning', 'NumericalInverseHermite', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binom_test', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'dirichlet', 'distributions', 'dlaplace', 'dweibull', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gilbrat', 'gmean', 'gompertz', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'randint', 'random_correlation', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'somersd', 'spearmanr', 'special_ortho_group', 'statlib', 'stats', 'studentized_range', 't', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'unitary_group', 'variation', 'vonmises', 'vonmises_line', 'wald', 'wasserstein_distance', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']\n"
     ]
    }
   ],
   "source": [
    "# t 를 불러온다\n",
    "\n",
    "print(scipy.stats.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3e8fecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.977625087231691 5.977625087231691\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# 표본 표준편차를 구하는 방법 (ddof=1로 설정)\n",
    "std = np.std(df.height, ddof=1)\n",
    "std1 = df.height.std()\n",
    "print(std, std1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e3e8df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on t_gen in module scipy.stats._continuous_distns object:\n",
      "\n",
      "class t_gen(scipy.stats._distn_infrastructure.rv_continuous)\n",
      " |  t_gen(momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, extradoc=None, seed=None)\n",
      " |  \n",
      " |  A Student's t continuous random variable.\n",
      " |  \n",
      " |  For the noncentral t distribution, see `nct`.\n",
      " |  \n",
      " |  %(before_notes)s\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  nct\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The probability density function for `t` is:\n",
      " |  \n",
      " |  .. math::\n",
      " |  \n",
      " |      f(x, \\nu) = \\frac{\\Gamma((\\nu+1)/2)}\n",
      " |                      {\\sqrt{\\pi \\nu} \\Gamma(\\nu/2)}\n",
      " |                  (1+x^2/\\nu)^{-(\\nu+1)/2}\n",
      " |  \n",
      " |  where :math:`x` is a real number and the degrees of freedom parameter\n",
      " |  :math:`\\nu` (denoted ``df`` in the implementation) satisfies\n",
      " |  :math:`\\nu > 0`. :math:`\\Gamma` is the gamma function\n",
      " |  (`scipy.special.gamma`).\n",
      " |  \n",
      " |  %(after_notes)s\n",
      " |  \n",
      " |  %(example)s\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      t_gen\n",
      " |      scipy.stats._distn_infrastructure.rv_continuous\n",
      " |      scipy.stats._distn_infrastructure.rv_generic\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods inherited from scipy.stats._distn_infrastructure.rv_continuous:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __init__(self, momtype=1, a=None, b=None, xtol=1e-14, badvalue=None, name=None, longname=None, shapes=None, extradoc=None, seed=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  cdf(self, x, *args, **kwds)\n",
      " |      Cumulative distribution function of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          quantiles\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      cdf : ndarray\n",
      " |          Cumulative distribution function evaluated at `x`\n",
      " |  \n",
      " |  expect(self, func=None, args=(), loc=0, scale=1, lb=None, ub=None, conditional=False, **kwds)\n",
      " |      Calculate expected value of a function with respect to the\n",
      " |      distribution by numerical integration.\n",
      " |      \n",
      " |      The expected value of a function ``f(x)`` with respect to a\n",
      " |      distribution ``dist`` is defined as::\n",
      " |      \n",
      " |                  ub\n",
      " |          E[f(x)] = Integral(f(x) * dist.pdf(x)),\n",
      " |                  lb\n",
      " |      \n",
      " |      where ``ub`` and ``lb`` are arguments and ``x`` has the ``dist.pdf(x)``\n",
      " |      distribution. If the bounds ``lb`` and ``ub`` correspond to the\n",
      " |      support of the distribution, e.g. ``[-inf, inf]`` in the default\n",
      " |      case, then the integral is the unrestricted expectation of ``f(x)``.\n",
      " |      Also, the function ``f(x)`` may be defined such that ``f(x)`` is ``0``\n",
      " |      outside a finite interval in which case the expectation is\n",
      " |      calculated within the finite range ``[lb, ub]``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      func : callable, optional\n",
      " |          Function for which integral is calculated. Takes only one argument.\n",
      " |          The default is the identity mapping f(x) = x.\n",
      " |      args : tuple, optional\n",
      " |          Shape parameters of the distribution.\n",
      " |      loc : float, optional\n",
      " |          Location parameter (default=0).\n",
      " |      scale : float, optional\n",
      " |          Scale parameter (default=1).\n",
      " |      lb, ub : scalar, optional\n",
      " |          Lower and upper bound for integration. Default is set to the\n",
      " |          support of the distribution.\n",
      " |      conditional : bool, optional\n",
      " |          If True, the integral is corrected by the conditional probability\n",
      " |          of the integration interval.  The return value is the expectation\n",
      " |          of the function, conditional on being in the given interval.\n",
      " |          Default is False.\n",
      " |      \n",
      " |      Additional keyword arguments are passed to the integration routine.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      expect : float\n",
      " |          The calculated expected value.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The integration behavior of this function is inherited from\n",
      " |      `scipy.integrate.quad`. Neither this function nor\n",
      " |      `scipy.integrate.quad` can verify whether the integral exists or is\n",
      " |      finite. For example ``cauchy(0).mean()`` returns ``np.nan`` and\n",
      " |      ``cauchy(0).expect()`` returns ``0.0``.\n",
      " |      \n",
      " |      Likewise, the accuracy of results is not verified by the function.\n",
      " |      `scipy.integrate.quad` is typically reliable for integrals that are\n",
      " |      numerically favorable, but it is not guaranteed to converge\n",
      " |      to a correct value for all possible intervals and integrands. This\n",
      " |      function is provided for convenience; for critical applications,\n",
      " |      check results against other integration methods.\n",
      " |      \n",
      " |      The function is not vectorized.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      To understand the effect of the bounds of integration consider\n",
      " |      \n",
      " |      >>> from scipy.stats import expon\n",
      " |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0)\n",
      " |      0.6321205588285578\n",
      " |      \n",
      " |      This is close to\n",
      " |      \n",
      " |      >>> expon(1).cdf(2.0) - expon(1).cdf(0.0)\n",
      " |      0.6321205588285577\n",
      " |      \n",
      " |      If ``conditional=True``\n",
      " |      \n",
      " |      >>> expon(1).expect(lambda x: 1, lb=0.0, ub=2.0, conditional=True)\n",
      " |      1.0000000000000002\n",
      " |      \n",
      " |      The slight deviation from 1 is due to numerical integration.\n",
      " |  \n",
      " |  fit(self, data, *args, **kwds)\n",
      " |      Return estimates of shape (if applicable), location, and scale\n",
      " |      parameters from data. The default estimation method is Maximum\n",
      " |      Likelihood Estimation (MLE), but Method of Moments (MM)\n",
      " |      is also available.\n",
      " |      \n",
      " |      Starting estimates for\n",
      " |      the fit are given by input arguments; for any arguments not provided\n",
      " |      with starting estimates, ``self._fitstart(data)`` is called to generate\n",
      " |      such.\n",
      " |      \n",
      " |      One can hold some parameters fixed to specific values by passing in\n",
      " |      keyword arguments ``f0``, ``f1``, ..., ``fn`` (for shape parameters)\n",
      " |      and ``floc`` and ``fscale`` (for location and scale parameters,\n",
      " |      respectively).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : array_like\n",
      " |          Data to use in estimating the distribution parameters.\n",
      " |      arg1, arg2, arg3,... : floats, optional\n",
      " |          Starting value(s) for any shape-characterizing arguments (those not\n",
      " |          provided will be determined by a call to ``_fitstart(data)``).\n",
      " |          No default value.\n",
      " |      **kwds : floats, optional\n",
      " |          - `loc`: initial guess of the distribution's location parameter.\n",
      " |          - `scale`: initial guess of the distribution's scale parameter.\n",
      " |      \n",
      " |          Special keyword arguments are recognized as holding certain\n",
      " |          parameters fixed:\n",
      " |      \n",
      " |          - f0...fn : hold respective shape parameters fixed.\n",
      " |            Alternatively, shape parameters to fix can be specified by name.\n",
      " |            For example, if ``self.shapes == \"a, b\"``, ``fa`` and ``fix_a``\n",
      " |            are equivalent to ``f0``, and ``fb`` and ``fix_b`` are\n",
      " |            equivalent to ``f1``.\n",
      " |      \n",
      " |          - floc : hold location parameter fixed to specified value.\n",
      " |      \n",
      " |          - fscale : hold scale parameter fixed to specified value.\n",
      " |      \n",
      " |          - optimizer : The optimizer to use.\n",
      " |            The optimizer must take ``func``,\n",
      " |            and starting position as the first two arguments,\n",
      " |            plus ``args`` (for extra arguments to pass to the\n",
      " |            function to be optimized) and ``disp=0`` to suppress\n",
      " |            output as keyword arguments.\n",
      " |      \n",
      " |          - method : The method to use. The default is \"MLE\" (Maximum\n",
      " |            Likelihood Estimate); \"MM\" (Method of Moments)\n",
      " |            is also available.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      TypeError, ValueError\n",
      " |          If an input is invalid\n",
      " |      `~scipy.stats.FitError`\n",
      " |          If fitting fails or the fit produced would be invalid\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      parameter_tuple : tuple of floats\n",
      " |          Estimates for any shape parameters (if applicable),\n",
      " |          followed by those for location and scale.\n",
      " |          For most random variables, shape statistics\n",
      " |          will be returned, but there are exceptions (e.g. ``norm``).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      With ``method=\"MLE\"`` (default), the fit is computed by minimizing\n",
      " |      the negative log-likelihood function. A large, finite penalty\n",
      " |      (rather than infinite negative log-likelihood) is applied for\n",
      " |      observations beyond the support of the distribution.\n",
      " |      \n",
      " |      With ``method=\"MM\"``, the fit is computed by minimizing the L2 norm\n",
      " |      of the relative errors between the first *k* raw (about zero) data\n",
      " |      moments and the corresponding distribution moments, where *k* is the\n",
      " |      number of non-fixed parameters.\n",
      " |      More precisely, the objective function is::\n",
      " |      \n",
      " |          (((data_moments - dist_moments)\n",
      " |            / np.maximum(np.abs(data_moments), 1e-8))**2).sum()\n",
      " |      \n",
      " |      where the constant ``1e-8`` avoids division by zero in case of\n",
      " |      vanishing data moments. Typically, this error norm can be reduced to\n",
      " |      zero.\n",
      " |      Note that the standard method of moments can produce parameters for\n",
      " |      which some data are outside the support of the fitted distribution;\n",
      " |      this implementation does nothing to prevent this.\n",
      " |      \n",
      " |      For either method,\n",
      " |      the returned answer is not guaranteed to be globally optimal; it\n",
      " |      may only be locally optimal, or the optimization may fail altogether.\n",
      " |      If the data contain any of ``np.nan``, ``np.inf``, or ``-np.inf``,\n",
      " |      the `fit` method will raise a ``RuntimeError``.\n",
      " |      \n",
      " |      Examples\n",
      " |      --------\n",
      " |      \n",
      " |      Generate some data to fit: draw random variates from the `beta`\n",
      " |      distribution\n",
      " |      \n",
      " |      >>> from scipy.stats import beta\n",
      " |      >>> a, b = 1., 2.\n",
      " |      >>> x = beta.rvs(a, b, size=1000)\n",
      " |      \n",
      " |      Now we can fit all four parameters (``a``, ``b``, ``loc``\n",
      " |      and ``scale``):\n",
      " |      \n",
      " |      >>> a1, b1, loc1, scale1 = beta.fit(x)\n",
      " |      \n",
      " |      We can also use some prior knowledge about the dataset: let's keep\n",
      " |      ``loc`` and ``scale`` fixed:\n",
      " |      \n",
      " |      >>> a1, b1, loc1, scale1 = beta.fit(x, floc=0, fscale=1)\n",
      " |      >>> loc1, scale1\n",
      " |      (0, 1)\n",
      " |      \n",
      " |      We can also keep shape parameters fixed by using ``f``-keywords. To\n",
      " |      keep the zero-th shape parameter ``a`` equal 1, use ``f0=1`` or,\n",
      " |      equivalently, ``fa=1``:\n",
      " |      \n",
      " |      >>> a1, b1, loc1, scale1 = beta.fit(x, fa=1, floc=0, fscale=1)\n",
      " |      >>> a1\n",
      " |      1\n",
      " |      \n",
      " |      Not all distributions return estimates for the shape parameters.\n",
      " |      ``norm`` for example just returns estimates for location and scale:\n",
      " |      \n",
      " |      >>> from scipy.stats import norm\n",
      " |      >>> x = norm.rvs(a, b, size=1000, random_state=123)\n",
      " |      >>> loc1, scale1 = norm.fit(x)\n",
      " |      >>> loc1, scale1\n",
      " |      (0.92087172783841631, 2.0015750750324668)\n",
      " |  \n",
      " |  fit_loc_scale(self, data, *args)\n",
      " |      Estimate loc and scale parameters from data using 1st and 2nd moments.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : array_like\n",
      " |          Data to fit.\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      Lhat : float\n",
      " |          Estimated location parameter for the data.\n",
      " |      Shat : float\n",
      " |          Estimated scale parameter for the data.\n",
      " |  \n",
      " |  isf(self, q, *args, **kwds)\n",
      " |      Inverse survival function (inverse of `sf`) at q of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : array_like\n",
      " |          upper tail probability\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      x : ndarray or scalar\n",
      " |          Quantile corresponding to the upper tail probability q.\n",
      " |  \n",
      " |  logcdf(self, x, *args, **kwds)\n",
      " |      Log of the cumulative distribution function at x of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          quantiles\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logcdf : array_like\n",
      " |          Log of the cumulative distribution function evaluated at x\n",
      " |  \n",
      " |  logpdf(self, x, *args, **kwds)\n",
      " |      Log of the probability density function at x of the given RV.\n",
      " |      \n",
      " |      This uses a more numerically accurate calculation if available.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          quantiles\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logpdf : array_like\n",
      " |          Log of the probability density function evaluated at x\n",
      " |  \n",
      " |  logsf(self, x, *args, **kwds)\n",
      " |      Log of the survival function of the given RV.\n",
      " |      \n",
      " |      Returns the log of the \"survival function,\" defined as (1 - `cdf`),\n",
      " |      evaluated at `x`.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          quantiles\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      logsf : ndarray\n",
      " |          Log of the survival function evaluated at `x`.\n",
      " |  \n",
      " |  pdf(self, x, *args, **kwds)\n",
      " |      Probability density function at x of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          quantiles\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      pdf : ndarray\n",
      " |          Probability density function evaluated at x\n",
      " |  \n",
      " |  ppf(self, q, *args, **kwds)\n",
      " |      Percent point function (inverse of `cdf`) at q of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      q : array_like\n",
      " |          lower tail probability\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      x : array_like\n",
      " |          quantile corresponding to the lower tail probability q.\n",
      " |  \n",
      " |  sf(self, x, *args, **kwds)\n",
      " |      Survival function (1 - `cdf`) at x of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      x : array_like\n",
      " |          quantiles\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      sf : array_like\n",
      " |          Survival function evaluated at x\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
      " |  \n",
      " |  __call__(self, *args, **kwds)\n",
      " |      Freeze the distribution for the given arguments.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution.  Should include all\n",
      " |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rv_frozen : rv_frozen instance\n",
      " |          The frozen distribution.\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  entropy(self, *args, **kwds)\n",
      " |      Differential entropy of the RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information).\n",
      " |      loc : array_like, optional\n",
      " |          Location parameter (default=0).\n",
      " |      scale : array_like, optional  (continuous distributions only).\n",
      " |          Scale parameter (default=1).\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      Entropy is defined base `e`:\n",
      " |      \n",
      " |      >>> drv = rv_discrete(values=((0, 1), (0.5, 0.5)))\n",
      " |      >>> np.allclose(drv.entropy(), np.log(2.0))\n",
      " |      True\n",
      " |  \n",
      " |  freeze(self, *args, **kwds)\n",
      " |      Freeze the distribution for the given arguments.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution.  Should include all\n",
      " |          the non-optional arguments, may include ``loc`` and ``scale``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rv_frozen : rv_frozen instance\n",
      " |          The frozen distribution.\n",
      " |  \n",
      " |  interval(self, confidence=None, *args, **kwds)\n",
      " |      Confidence interval with equal areas around the median.\n",
      " |      \n",
      " |      .. deprecated:: 1.9.0\n",
      " |         Parameter `alpha` is replaced by parameter `confidence` to avoid\n",
      " |         name collisions with the shape parameter `alpha` of some\n",
      " |         distributions. Parameter `alpha` will be removed in SciPy 1.11.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      confidence : array_like of float\n",
      " |          Probability that an rv will be drawn from the returned range.\n",
      " |          Each value should be in the range [0, 1].\n",
      " |      arg1, arg2, ... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information).\n",
      " |      loc : array_like, optional\n",
      " |          location parameter, Default is 0.\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter, Default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a, b : ndarray of float\n",
      " |          end-points of range that contain ``100 * alpha %`` of the rv's\n",
      " |          possible values.\n",
      " |  \n",
      " |  mean(self, *args, **kwds)\n",
      " |      Mean of the distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      mean : float\n",
      " |          the mean of the distribution\n",
      " |  \n",
      " |  median(self, *args, **kwds)\n",
      " |      Median of the distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          Location parameter, Default is 0.\n",
      " |      scale : array_like, optional\n",
      " |          Scale parameter, Default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      median : float\n",
      " |          The median of the distribution.\n",
      " |      \n",
      " |      See Also\n",
      " |      --------\n",
      " |      rv_discrete.ppf\n",
      " |          Inverse of the CDF\n",
      " |  \n",
      " |  moment(self, order=None, *args, **kwds)\n",
      " |      non-central moment of distribution of specified order.\n",
      " |      \n",
      " |      .. deprecated:: 1.9.0\n",
      " |         Parameter `n` is replaced by parameter `order` to avoid name\n",
      " |         collisions with the shape parameter `n` of several distributions.\n",
      " |         Parameter `n` will be removed in SciPy 1.11.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      order : int, order >= 1\n",
      " |          Order of moment.\n",
      " |      arg1, arg2, arg3,... : float\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information).\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |  \n",
      " |  nnlf(self, theta, x)\n",
      " |      Negative loglikelihood function.\n",
      " |      Notes\n",
      " |      -----\n",
      " |      This is ``-sum(log pdf(x, theta), axis=0)`` where `theta` are the\n",
      " |      parameters (including loc and scale).\n",
      " |  \n",
      " |  rvs(self, *args, **kwds)\n",
      " |      Random variates of given type.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information).\n",
      " |      loc : array_like, optional\n",
      " |          Location parameter (default=0).\n",
      " |      scale : array_like, optional\n",
      " |          Scale parameter (default=1).\n",
      " |      size : int or tuple of ints, optional\n",
      " |          Defining number of random variates (default is 1).\n",
      " |      random_state : {None, int, `numpy.random.Generator`,\n",
      " |                      `numpy.random.RandomState`}, optional\n",
      " |      \n",
      " |          If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      " |          singleton is used.\n",
      " |          If `seed` is an int, a new ``RandomState`` instance is used,\n",
      " |          seeded with `seed`.\n",
      " |          If `seed` is already a ``Generator`` or ``RandomState`` instance\n",
      " |          then that instance is used.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      rvs : ndarray or scalar\n",
      " |          Random variates of given `size`.\n",
      " |  \n",
      " |  stats(self, *args, **kwds)\n",
      " |      Some statistics of the given RV.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional (continuous RVs only)\n",
      " |          scale parameter (default=1)\n",
      " |      moments : str, optional\n",
      " |          composed of letters ['mvsk'] defining which moments to compute:\n",
      " |          'm' = mean,\n",
      " |          'v' = variance,\n",
      " |          's' = (Fisher's) skew,\n",
      " |          'k' = (Fisher's) kurtosis.\n",
      " |          (default is 'mv')\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      stats : sequence\n",
      " |          of requested moments.\n",
      " |  \n",
      " |  std(self, *args, **kwds)\n",
      " |      Standard deviation of the distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      std : float\n",
      " |          standard deviation of the distribution\n",
      " |  \n",
      " |  support(self, *args, **kwargs)\n",
      " |      Support of the distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, ... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information).\n",
      " |      loc : array_like, optional\n",
      " |          location parameter, Default is 0.\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter, Default is 1.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      a, b : array_like\n",
      " |          end-points of the distribution's support.\n",
      " |  \n",
      " |  var(self, *args, **kwds)\n",
      " |      Variance of the distribution.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      arg1, arg2, arg3,... : array_like\n",
      " |          The shape parameter(s) for the distribution (see docstring of the\n",
      " |          instance object for more information)\n",
      " |      loc : array_like, optional\n",
      " |          location parameter (default=0)\n",
      " |      scale : array_like, optional\n",
      " |          scale parameter (default=1)\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      var : float\n",
      " |          the variance of the distribution\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from scipy.stats._distn_infrastructure.rv_generic:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  random_state\n",
      " |      Get or set the generator object for generating random variates.\n",
      " |      \n",
      " |      If `seed` is None (or `np.random`), the `numpy.random.RandomState`\n",
      " |      singleton is used.\n",
      " |      If `seed` is an int, a new ``RandomState`` instance is used,\n",
      " |      seeded with `seed`.\n",
      " |      If `seed` is already a ``Generator`` or ``RandomState`` instance then\n",
      " |      that instance is used.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54922c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0049\n"
     ]
    }
   ],
   "source": [
    "# 신뢰수준\n",
    "confidence_level = 0.95\n",
    "\n",
    "# 자유도\n",
    "ddof = len(df.height) - 1\n",
    "\n",
    "# t분포의 양쪽 꼬리에서의 t값\n",
    "t_value = t.ppf((1+confidence_level)/2, ddof)\n",
    "\n",
    "print(round(t_value, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc38aa1",
   "metadata": {},
   "source": [
    "# 3-1-c\n",
    "95% 신뢰구간을 구하여라(print(lower,upper) 방식으로 출력, 각각의 값은 소숫점 이하 3째자리까지)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5dc023ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "168.321 171.553\n"
     ]
    }
   ],
   "source": [
    "# 95% 신뢰구간 계산\n",
    "\n",
    "mean = round(df['height'].mean(), 3)\n",
    "n = len(df.height)\n",
    "\n",
    "lower = round(mean - t_value * std / np.sqrt(n),3)\n",
    "upper = round(mean + t_value * std / np.sqrt(n),3)\n",
    "\n",
    "print(lower,upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8033019e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b80bea3",
   "metadata": {},
   "source": [
    "# 3-2번\n",
    "공장간의 제품 길이 차이가 유의미한지 확인 하려한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4f59e9b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>84.179434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>93.971003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>62.140957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>72.911723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>79.252747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ID      value\n",
       "0  A  84.179434\n",
       "1  A  93.971003\n",
       "2  A  62.140957\n",
       "3  A  72.911723\n",
       "4  A  79.252747"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "df2= pd.read_csv('https://raw.githubusercontent.com/Datamanim/datarepo/main/krdatacertificate/e5_p3_2.csv')\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b17fa1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30 entries, 0 to 29\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   ID      30 non-null     object \n",
      " 1   value   30 non-null     float64\n",
      "dtypes: float64(1), object(1)\n",
      "memory usage: 608.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813ffe9",
   "metadata": {},
   "source": [
    "# 3-2-a\n",
    "3 그룹의 데이터에 대해 크루스칼-왈리스 검정을 사용하여 검정 통계량을 반올림하여 소숫점 이하 3자리까지 구하여라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d0244f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function kruskal in module scipy.stats._stats_py:\n",
      "\n",
      "kruskal(*samples, nan_policy='propagate', axis=0, keepdims=False)\n",
      "    Compute the Kruskal-Wallis H-test for independent samples.\n",
      "    \n",
      "    The Kruskal-Wallis H-test tests the null hypothesis that the population\n",
      "    median of all of the groups are equal.  It is a non-parametric version of\n",
      "    ANOVA.  The test works on 2 or more independent samples, which may have\n",
      "    different sizes.  Note that rejecting the null hypothesis does not\n",
      "    indicate which of the groups differs.  Post hoc comparisons between\n",
      "    groups are required to determine which groups are different.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    sample1, sample2, ... : array_like\n",
      "        Two or more arrays with the sample measurements can be given as\n",
      "        arguments. Samples must be one-dimensional.\n",
      "    nan_policy : {'propagate', 'omit', 'raise'}\n",
      "        Defines how to handle input NaNs.\n",
      "        \n",
      "        - ``propagate``: if a NaN is present in the axis slice (e.g. row) along\n",
      "          which the  statistic is computed, the corresponding entry of the output\n",
      "          will be NaN.\n",
      "        - ``omit``: NaNs will be omitted when performing the calculation.\n",
      "          If insufficient data remains in the axis slice along which the\n",
      "          statistic is computed, the corresponding entry of the output will be\n",
      "          NaN.\n",
      "        - ``raise``: if a NaN is present, a ``ValueError`` will be raised.\n",
      "    axis : int or None, default: 0\n",
      "        If an int, the axis of the input along which to compute the statistic.\n",
      "        The statistic of each axis-slice (e.g. row) of the input will appear in a\n",
      "        corresponding element of the output.\n",
      "        If ``None``, the input will be raveled before computing the statistic.\n",
      "    keepdims : bool, default: False\n",
      "        If this is set to True, the axes which are reduced are left\n",
      "        in the result as dimensions with size one. With this option,\n",
      "        the result will broadcast correctly against the input array.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float\n",
      "        The Kruskal-Wallis H statistic, corrected for ties.\n",
      "    pvalue : float\n",
      "        The p-value for the test using the assumption that H has a chi\n",
      "        square distribution. The p-value returned is the survival function of\n",
      "        the chi square distribution evaluated at H.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    \n",
      "    :func:`f_oneway`\n",
      "        1-way ANOVA.\n",
      "    :func:`mannwhitneyu`\n",
      "        Mann-Whitney rank test on two samples.\n",
      "    :func:`friedmanchisquare`\n",
      "        Friedman test for repeated measurements.\n",
      "    \n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    Due to the assumption that H has a chi square distribution, the number\n",
      "    of samples in each group must not be too small.  A typical rule is\n",
      "    that each sample must have at least 5 measurements.\n",
      "    \n",
      "    Beginning in SciPy 1.9, ``np.matrix`` inputs (not recommended for new\n",
      "    code) are converted to ``np.ndarray`` before the calculation is performed. In\n",
      "    this case, the output will be a scalar or ``np.ndarray`` of appropriate shape\n",
      "    rather than a 2D ``np.matrix``. Similarly, while masked elements of masked\n",
      "    arrays are ignored, the output will be a scalar or ``np.ndarray`` rather than a\n",
      "    masked array with ``mask=False``.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    .. [1] W. H. Kruskal & W. W. Wallis, \"Use of Ranks in\n",
      "       One-Criterion Variance Analysis\", Journal of the American Statistical\n",
      "       Association, Vol. 47, Issue 260, pp. 583-621, 1952.\n",
      "    .. [2] https://en.wikipedia.org/wiki/Kruskal-Wallis_one-way_analysis_of_variance\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from scipy import stats\n",
      "    >>> x = [1, 3, 5, 7, 9]\n",
      "    >>> y = [2, 4, 6, 8, 10]\n",
      "    >>> stats.kruskal(x, y)\n",
      "    KruskalResult(statistic=0.2727272727272734, pvalue=0.6015081344405895)\n",
      "    \n",
      "    >>> x = [1, 1, 1]\n",
      "    >>> y = [2, 2, 2]\n",
      "    >>> z = [2, 2]\n",
      "    >>> stats.kruskal(x, y, z)\n",
      "    KruskalResult(statistic=7.0, pvalue=0.0301973834223185)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(kruskal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc10548c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.521\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import kruskal\n",
    "\n",
    "\"\"\"\n",
    "크루스왈리스 검정은 모든 그룹의 중앙값이 같은지 검정한다.\n",
    "2개 이상의 사이즈가 서로 다른 그룹을 대상으로 한다.\n",
    "\"\"\"\n",
    "\n",
    "# 넘파이 어레이 형태로 저장\n",
    "a = df2[df2.ID=='A'].value.values\n",
    "b = df2[df2.ID=='B'].value.values\n",
    "c = df2[df2.ID=='C'].value.values\n",
    "\n",
    "# statistic, p-value 반환\n",
    "s, p = kruskal(a, b, c)\n",
    "\n",
    "print(round(s, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8c8e01",
   "metadata": {},
   "source": [
    "# 3-2-b\n",
    "3 그룹의 데이터에 대해 크루스칼-왈리스 검정을 사용하여 p-value를 반올림하여 소숫점 이하 3자리까지 구하여라. 귀무가설과 대립가설중 0.05 유의수준에서 유의한 가설을 출력하라"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "924e95ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.038\n",
      "대립\n"
     ]
    }
   ],
   "source": [
    "# 0.05보다 작으므로 귀무가설을 기각한다.\n",
    "\n",
    "round_p = round(p,3)\n",
    "print(round_p)\n",
    "print('대립')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729f4122",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa887c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d7761a5",
   "metadata": {},
   "source": [
    "함수 기능 확인 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61a3579f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['calibration', 'cluster', 'covariance', 'cross_decomposition', 'datasets', 'decomposition', 'dummy', 'ensemble', 'exceptions', 'experimental', 'externals', 'feature_extraction', 'feature_selection', 'gaussian_process', 'inspection', 'isotonic', 'kernel_approximation', 'kernel_ridge', 'linear_model', 'manifold', 'metrics', 'mixture', 'model_selection', 'multiclass', 'multioutput', 'naive_bayes', 'neighbors', 'neural_network', 'pipeline', 'preprocessing', 'random_projection', 'semi_supervised', 'svm', 'tree', 'discriminant_analysis', 'impute', 'compose', 'clone', 'get_config', 'set_config', 'config_context', 'show_versions']\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "\n",
    "print(sklearn.__all__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1f2ab86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BaseEnsemble', 'RandomForestClassifier', 'RandomForestRegressor', 'RandomTreesEmbedding', 'ExtraTreesClassifier', 'ExtraTreesRegressor', 'BaggingClassifier', 'BaggingRegressor', 'IsolationForest', 'GradientBoostingClassifier', 'GradientBoostingRegressor', 'AdaBoostClassifier', 'AdaBoostRegressor', 'VotingClassifier', 'VotingRegressor', 'StackingClassifier', 'StackingRegressor', 'HistGradientBoostingClassifier', 'HistGradientBoostingRegressor']\n"
     ]
    }
   ],
   "source": [
    "import sklearn.ensemble\n",
    "\n",
    "print(sklearn.ensemble.__all__) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8024794a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on RandomForestRegressor in module sklearn.ensemble._forest object:\n",
      "\n",
      "class RandomForestRegressor(ForestRegressor)\n",
      " |  RandomForestRegressor(n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |  \n",
      " |  A random forest regressor.\n",
      " |  \n",
      " |  A random forest is a meta estimator that fits a number of classifying\n",
      " |  decision trees on various sub-samples of the dataset and uses averaging\n",
      " |  to improve the predictive accuracy and control over-fitting.\n",
      " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
      " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      " |  each tree.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <forest>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  n_estimators : int, default=100\n",
      " |      The number of trees in the forest.\n",
      " |  \n",
      " |      .. versionchanged:: 0.22\n",
      " |         The default value of ``n_estimators`` changed from 10 to 100\n",
      " |         in 0.22.\n",
      " |  \n",
      " |  criterion : {\"squared_error\", \"absolute_error\", \"poisson\"},             default=\"squared_error\"\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are \"squared_error\" for the mean squared error, which is equal to\n",
      " |      variance reduction as feature selection criterion, \"absolute_error\"\n",
      " |      for the mean absolute error, and \"poisson\" which uses reduction in\n",
      " |      Poisson deviance to find splits.\n",
      " |      Training using \"absolute_error\" is significantly slower\n",
      " |      than when using \"squared_error\".\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |         Mean Absolute Error (MAE) criterion.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |         Poisson criterion.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion \"mse\" was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion=\"squared_error\"` which is equivalent.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion \"mae\" was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion=\"absolute_error\"` which is equivalent.\n",
      " |  \n",
      " |  max_depth : int, default=None\n",
      " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
      " |      all leaves are pure or until all leaves contain less than\n",
      " |      min_samples_split samples.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_features : {\"sqrt\", \"log2\", None}, int or float, default=1.0\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `max(1, int(max_features * n_features_in_))` features are considered at each\n",
      " |        split.\n",
      " |      - If \"auto\", then `max_features=n_features`.\n",
      " |      - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
      " |      - If None or 1.0, then `max_features=n_features`.\n",
      " |  \n",
      " |      .. note::\n",
      " |          The default of 1.0 is equivalent to bagged trees and more\n",
      " |          randomness can be achieved by setting smaller values, e.g. 0.3.\n",
      " |  \n",
      " |      .. versionchanged:: 1.1\n",
      " |          The default of `max_features` changed from `\"auto\"` to 1.0.\n",
      " |  \n",
      " |      .. deprecated:: 1.1\n",
      " |          The `\"auto\"` option was deprecated in 1.1 and will be removed\n",
      " |          in 1.3.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  bootstrap : bool, default=True\n",
      " |      Whether bootstrap samples are used when building trees. If False, the\n",
      " |      whole dataset is used to build each tree.\n",
      " |  \n",
      " |  oob_score : bool, default=False\n",
      " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
      " |      Only available if bootstrap=True.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
      " |      <n_jobs>` for more details.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls both the randomness of the bootstrapping of the samples used\n",
      " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
      " |      features to consider when looking for the best split at each node\n",
      " |      (if ``max_features < n_features``).\n",
      " |      See :term:`Glossary <random_state>` for details.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Controls the verbosity when fitting and predicting.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
      " |      new forest. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  max_samples : int or float, default=None\n",
      " |      If bootstrap is True, the number of samples to draw from X\n",
      " |      to train each base estimator.\n",
      " |  \n",
      " |      - If None (default), then draw `X.shape[0]` samples.\n",
      " |      - If int, then draw `max_samples` samples.\n",
      " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  base_estimator_ : DecisionTreeRegressor\n",
      " |      The child estimator template used to create the collection of fitted\n",
      " |      sub-estimators.\n",
      " |  \n",
      " |  estimators_ : list of DecisionTreeRegressor\n",
      " |      The collection of fitted sub-estimators.\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of features when ``fit`` is performed.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      The number of outputs when ``fit`` is performed.\n",
      " |  \n",
      " |  oob_score_ : float\n",
      " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  oob_prediction_ : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |      Prediction computed with out-of-bag estimate on the training set.\n",
      " |      This attribute exists only when ``oob_score`` is True.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.tree.DecisionTreeRegressor : A decision tree regressor.\n",
      " |  sklearn.ensemble.ExtraTreesRegressor : Ensemble of extremely randomized\n",
      " |      tree regressors.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The default values for the parameters controlling the size of the trees\n",
      " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      " |  unpruned trees which can potentially be very large on some data sets. To\n",
      " |  reduce memory consumption, the complexity and size of the trees should be\n",
      " |  controlled by setting those parameter values.\n",
      " |  \n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data,\n",
      " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      " |  of the criterion is identical for several splits enumerated during the\n",
      " |  search of the best split. To obtain a deterministic behaviour during\n",
      " |  fitting, ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      " |  rather than ``n_features / 3``. The latter was originally suggested in\n",
      " |  [1], whereas the former was more recently justified empirically in [2].\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      " |  \n",
      " |  .. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      " |         trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from sklearn.ensemble import RandomForestRegressor\n",
      " |  >>> from sklearn.datasets import make_regression\n",
      " |  >>> X, y = make_regression(n_features=4, n_informative=2,\n",
      " |  ...                        random_state=0, shuffle=False)\n",
      " |  >>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      " |  >>> regr.fit(X, y)\n",
      " |  RandomForestRegressor(...)\n",
      " |  >>> print(regr.predict([[0, 0, 0, 0]]))\n",
      " |  [-8.32987858]\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      RandomForestRegressor\n",
      " |      ForestRegressor\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      BaseForest\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, n_estimators=100, *, criterion='squared_error', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=1.0, max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, ccp_alpha=0.0, max_samples=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from ForestRegressor:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict regression target for X.\n",
      " |      \n",
      " |      The predicted regression target of an input sample is computed as the\n",
      " |      mean predicted regression targets of the trees in the forest.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination of the prediction.\n",
      " |      \n",
      " |      The coefficient of determination :math:`R^2` is defined as\n",
      " |      :math:`(1 - \\frac{u}{v})`, where :math:`u` is the residual\n",
      " |      sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`\n",
      " |      is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always predicts\n",
      " |      the expected value of `y`, disregarding the input features, would get\n",
      " |      a :math:`R^2` score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a precomputed\n",
      " |          kernel matrix or a list of generic objects instead with shape\n",
      " |          ``(n_samples, n_samples_fitted)``, where ``n_samples_fitted``\n",
      " |          is the number of samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          :math:`R^2` of ``self.predict(X)`` wrt. `y`.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The :math:`R^2` score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseForest:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the forest to X, return leaf indices.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
      " |          For each datapoint x in X and for each tree in the forest,\n",
      " |          return the index of the leaf x ends up in.\n",
      " |  \n",
      " |  decision_path(self, X)\n",
      " |      Return the decision path in the forest.\n",
      " |      \n",
      " |      .. versionadded:: 0.18\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
      " |          Return a node indicator matrix where non zero elements indicates\n",
      " |          that the samples goes through the nodes. The matrix is of CSR\n",
      " |          format.\n",
      " |      \n",
      " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
      " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
      " |          gives the indicator value for the i-th estimator.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Build a forest of trees from the training set (X, y).\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The training input samples. Internally, its dtype will be converted\n",
      " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
      " |          converted into a sparse ``csc_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from BaseForest:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |      \n",
      " |      Number of features when fitting the estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# help function으로 함수 이용법 확인\n",
    "print(help(sklearn.ensemble.RandomForestRegressor()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f6b0363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ConstantInputWarning', 'DegenerateDataWarning', 'FitError', 'NearConstantInputWarning', 'NumericalInverseHermite', 'alexandergovern', 'alpha', 'anderson', 'anderson_ksamp', 'anglit', 'ansari', 'arcsine', 'argus', 'barnard_exact', 'bartlett', 'bayes_mvs', 'bernoulli', 'beta', 'betabinom', 'betaprime', 'biasedurn', 'binned_statistic', 'binned_statistic_2d', 'binned_statistic_dd', 'binom', 'binom_test', 'binomtest', 'boltzmann', 'bootstrap', 'boschloo_exact', 'boxcox', 'boxcox_llf', 'boxcox_normmax', 'boxcox_normplot', 'bradford', 'brunnermunzel', 'burr', 'burr12', 'cauchy', 'chi', 'chi2', 'chi2_contingency', 'chisquare', 'circmean', 'circstd', 'circvar', 'combine_pvalues', 'contingency', 'cosine', 'cramervonmises', 'cramervonmises_2samp', 'crystalball', 'cumfreq', 'describe', 'dgamma', 'differential_entropy', 'dirichlet', 'distributions', 'dlaplace', 'dweibull', 'energy_distance', 'entropy', 'epps_singleton_2samp', 'erlang', 'expon', 'exponnorm', 'exponpow', 'exponweib', 'f', 'f_oneway', 'fatiguelife', 'find_repeats', 'fisher_exact', 'fisk', 'fit', 'fligner', 'foldcauchy', 'foldnorm', 'friedmanchisquare', 'gamma', 'gausshyper', 'gaussian_kde', 'genexpon', 'genextreme', 'gengamma', 'genhalflogistic', 'genhyperbolic', 'geninvgauss', 'genlogistic', 'gennorm', 'genpareto', 'geom', 'gibrat', 'gilbrat', 'gmean', 'gompertz', 'gstd', 'gumbel_l', 'gumbel_r', 'gzscore', 'halfcauchy', 'halfgennorm', 'halflogistic', 'halfnorm', 'hmean', 'hypergeom', 'hypsecant', 'invgamma', 'invgauss', 'invweibull', 'invwishart', 'iqr', 'jarque_bera', 'johnsonsb', 'johnsonsu', 'kappa3', 'kappa4', 'kde', 'kendalltau', 'kruskal', 'ks_1samp', 'ks_2samp', 'ksone', 'kstat', 'kstatvar', 'kstest', 'kstwo', 'kstwobign', 'kurtosis', 'kurtosistest', 'laplace', 'laplace_asymmetric', 'levene', 'levy', 'levy_l', 'levy_stable', 'linregress', 'loggamma', 'logistic', 'loglaplace', 'lognorm', 'logser', 'loguniform', 'lomax', 'mannwhitneyu', 'matrix_normal', 'maxwell', 'median_abs_deviation', 'median_test', 'mielke', 'mode', 'moment', 'monte_carlo_test', 'mood', 'morestats', 'moyal', 'mstats', 'mstats_basic', 'mstats_extras', 'multinomial', 'multiscale_graphcorr', 'multivariate_hypergeom', 'multivariate_normal', 'multivariate_t', 'mvn', 'mvsdist', 'nakagami', 'nbinom', 'ncf', 'nchypergeom_fisher', 'nchypergeom_wallenius', 'nct', 'ncx2', 'nhypergeom', 'norm', 'normaltest', 'norminvgauss', 'obrientransform', 'ortho_group', 'page_trend_test', 'pareto', 'pearson3', 'pearsonr', 'percentileofscore', 'permutation_test', 'planck', 'pmean', 'pointbiserialr', 'poisson', 'power_divergence', 'powerlaw', 'powerlognorm', 'powernorm', 'ppcc_max', 'ppcc_plot', 'probplot', 'qmc', 'randint', 'random_correlation', 'rankdata', 'ranksums', 'rayleigh', 'rdist', 'recipinvgauss', 'reciprocal', 'relfreq', 'rice', 'rv_continuous', 'rv_discrete', 'rv_histogram', 'rvs_ratio_uniforms', 'scoreatpercentile', 'sem', 'semicircular', 'shapiro', 'siegelslopes', 'sigmaclip', 'skellam', 'skew', 'skewcauchy', 'skewnorm', 'skewtest', 'somersd', 'spearmanr', 'special_ortho_group', 'statlib', 'stats', 'studentized_range', 't', 'theilslopes', 'tiecorrect', 'tmax', 'tmean', 'tmin', 'trapezoid', 'trapz', 'triang', 'trim1', 'trim_mean', 'trimboth', 'truncexpon', 'truncnorm', 'truncweibull_min', 'tsem', 'tstd', 'ttest_1samp', 'ttest_ind', 'ttest_ind_from_stats', 'ttest_rel', 'tukey_hsd', 'tukeylambda', 'tvar', 'uniform', 'unitary_group', 'variation', 'vonmises', 'vonmises_line', 'wald', 'wasserstein_distance', 'weibull_max', 'weibull_min', 'weightedtau', 'wilcoxon', 'wishart', 'wrapcauchy', 'yeojohnson', 'yeojohnson_llf', 'yeojohnson_normmax', 'yeojohnson_normplot', 'yulesimon', 'zipf', 'zipfian', 'zmap', 'zscore']\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats\n",
    "\n",
    "print(scipy.stats.__all__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46c82b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function ttest_1samp in module scipy.stats._stats_py:\n",
      "\n",
      "ttest_1samp(a, popmean, axis=0, nan_policy='propagate', alternative='two-sided')\n",
      "    Calculate the T-test for the mean of ONE group of scores.\n",
      "    \n",
      "    This is a test for the null hypothesis that the expected value\n",
      "    (mean) of a sample of independent observations `a` is equal to the given\n",
      "    population mean, `popmean`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    a : array_like\n",
      "        Sample observation.\n",
      "    popmean : float or array_like\n",
      "        Expected value in null hypothesis. If array_like, then it must have the\n",
      "        same shape as `a` excluding the axis dimension.\n",
      "    axis : int or None, optional\n",
      "        Axis along which to compute test; default is 0. If None, compute over\n",
      "        the whole array `a`.\n",
      "    nan_policy : {'propagate', 'raise', 'omit'}, optional\n",
      "        Defines how to handle when input contains nan.\n",
      "        The following options are available (default is 'propagate'):\n",
      "    \n",
      "          * 'propagate': returns nan\n",
      "          * 'raise': throws an error\n",
      "          * 'omit': performs the calculations ignoring nan values\n",
      "    \n",
      "    alternative : {'two-sided', 'less', 'greater'}, optional\n",
      "        Defines the alternative hypothesis.\n",
      "        The following options are available (default is 'two-sided'):\n",
      "    \n",
      "        * 'two-sided': the mean of the underlying distribution of the sample\n",
      "          is different than the given population mean (`popmean`)\n",
      "        * 'less': the mean of the underlying distribution of the sample is\n",
      "          less than the given population mean (`popmean`)\n",
      "        * 'greater': the mean of the underlying distribution of the sample is\n",
      "          greater than the given population mean (`popmean`)\n",
      "    \n",
      "        .. versionadded:: 1.6.0\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    statistic : float or array\n",
      "        t-statistic.\n",
      "    pvalue : float or array\n",
      "        Two-sided p-value.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    The statistic is calculated as ``(np.mean(a) - popmean)/se``, where\n",
      "    ``se`` is the standard error. Therefore, the statistic will be positive\n",
      "    when the sample mean is greater than the population mean and negative when\n",
      "    the sample mean is less than the population mean.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    Suppose we wish to test the null hypothesis that the mean of a population\n",
      "    is equal to 0.5. We choose a confidence level of 99%; that is, we will\n",
      "    reject the null hypothesis in favor of the alternative if the p-value is\n",
      "    less than 0.01.\n",
      "    \n",
      "    When testing random variates from the standard uniform distribution, which\n",
      "    has a mean of 0.5, we expect the data to be consistent with the null\n",
      "    hypothesis most of the time.\n",
      "    \n",
      "    >>> from scipy import stats\n",
      "    >>> rng = np.random.default_rng()\n",
      "    >>> rvs = stats.uniform.rvs(size=50, random_state=rng)\n",
      "    >>> stats.ttest_1samp(rvs, popmean=0.5)\n",
      "    Ttest_1sampResult(statistic=2.456308468440, pvalue=0.017628209047638)\n",
      "    \n",
      "    As expected, the p-value of 0.017 is not below our threshold of 0.01, so\n",
      "    we cannot reject the null hypothesis.\n",
      "    \n",
      "    When testing data from the standard *normal* distribution, which has a mean\n",
      "    of 0, we would expect the null hypothesis to be rejected.\n",
      "    \n",
      "    >>> rvs = stats.norm.rvs(size=50, random_state=rng)\n",
      "    >>> stats.ttest_1samp(rvs, popmean=0.5)\n",
      "    Ttest_1sampResult(statistic=-7.433605518875, pvalue=1.416760157221e-09)\n",
      "    \n",
      "    Indeed, the p-value is lower than our threshold of 0.01, so we reject the\n",
      "    null hypothesis in favor of the default \"two-sided\" alternative: the mean\n",
      "    of the population is *not* equal to 0.5.\n",
      "    \n",
      "    However, suppose we were to test the null hypothesis against the\n",
      "    one-sided alternative that the mean of the population is *greater* than\n",
      "    0.5. Since the mean of the standard normal is less than 0.5, we would not\n",
      "    expect the null hypothesis to be rejected.\n",
      "    \n",
      "    >>> stats.ttest_1samp(rvs, popmean=0.5, alternative='greater')\n",
      "    Ttest_1sampResult(statistic=-7.433605518875, pvalue=0.99999999929)\n",
      "    \n",
      "    Unsurprisingly, with a p-value greater than our threshold, we would not\n",
      "    reject the null hypothesis.\n",
      "    \n",
      "    Note that when working with a confidence level of 99%, a true null\n",
      "    hypothesis will be rejected approximately 1% of the time.\n",
      "    \n",
      "    >>> rvs = stats.uniform.rvs(size=(100, 50), random_state=rng)\n",
      "    >>> res = stats.ttest_1samp(rvs, popmean=0.5, axis=1)\n",
      "    >>> np.sum(res.pvalue < 0.01)\n",
      "    1\n",
      "    \n",
      "    Indeed, even though all 100 samples above were drawn from the standard\n",
      "    uniform distribution, which *does* have a population mean of 0.5, we would\n",
      "    mistakenly reject the null hypothesis for one of them.\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(help(scipy.stats.ttest_1samp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a49250f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
